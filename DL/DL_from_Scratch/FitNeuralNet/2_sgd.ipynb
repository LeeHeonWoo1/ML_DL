{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법\n",
    "기계학습의 대부분은 학습 단계에서 최적의 매개변수를 찾아냅니다. 신경망 역시 최적의 매개변수(가중치, 편향)를 학습시에 찾아야 합니다. 이런 상황에서 기울기를 잘 이용해 함수의 최솟값을 찾으려는 것이 경사법입니다. \n",
    "\n",
    "#### 주의해야할 점\n",
    "각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기라는 것입니다. 하지만 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지, 즉 그쪽이 정말로 나아갈 방향인지는 보장할 수 없습니다. 경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동합니다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복합니다.. 이렇게 함수의 값을 점차 줄이는 것을 `경사하강법` 혹은 `경사상승법` 이라고 합니다.\n",
    "\n",
    "### 수식\n",
    "> x<sub>0</sub> = x<sub>0</sub> - ŋ * ∂f / ∂x<sub>0</sub> <br>\n",
    "x<sub>1</sub> = x<sub>1</sub> - ŋ * ∂f / ∂x<sub>1</sub>\n",
    "\n",
    "수식에서 기호 ŋ는 갱신하는 양을 나타냅니다. 이는 신경망에서는 `학습률`이라고 합니다. 한 번의 학습으로 얼마만큼 학습해야 할지, 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률입니다. 이 학습률은 너무 커서도 안되고 너무 작아도 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 경사법 구현\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mymodules.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for _ in range(step_num):\n",
    "        x_history.append( x.copy() ) # 개선되는 x값을 빈 리스트에 저장해서 히스토리를 만든다.\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "# 문제 : 경사법으로 f(x0, x1) = x0^2 + x1^2의 최솟값을 구하라\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0]) # init_x 정의\n",
    "\n",
    "lr = 0.1 # 학습률\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num) # 경사하강법 시행\n",
    "\n",
    "# 시각화\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show() # 기울기가 점점 작아지는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-2.58983747e+13, -1.29524862e+12]), array([[-3.00000000e+00,  4.00000000e+00],\n",
      "       [ 5.70000000e+01, -7.60000000e+01],\n",
      "       [-1.08300000e+03,  1.44400000e+03],\n",
      "       [ 2.05770000e+04, -2.74360000e+04],\n",
      "       [-3.90963008e+05,  5.21284002e+05],\n",
      "       [ 7.42829664e+06, -9.90439654e+06],\n",
      "       [-1.41137328e+08,  1.88183103e+08],\n",
      "       [ 2.68126267e+09, -3.57501690e+09],\n",
      "       [-5.09763373e+10,  6.79001831e+10],\n",
      "       [ 9.45170863e+11, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12],\n",
      "       [-2.58983747e+13, -1.29524862e+12]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-2.99999994,  3.99999992]),\n",
       " array([[-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  3.99999999],\n",
       "        [-3.        ,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999997,  3.99999997],\n",
       "        [-2.99999997,  3.99999997],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999995],\n",
       "        [-2.99999997,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999994,  3.99999993],\n",
       "        [-2.99999994,  3.99999993],\n",
       "        [-2.99999994,  3.99999992],\n",
       "        [-2.99999994,  3.99999992],\n",
       "        [-2.99999994,  3.99999992],\n",
       "        [-2.99999994,  3.99999992],\n",
       "        [-2.99999994,  3.99999992],\n",
       "        [-2.99999994,  3.99999992]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 크거나 작다면?\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x, 10.0, 100)) # 학습률이 너무 큰 경우 굉장히 큰 값으로 발산하고,\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x, 1e-10, 100) # 학습률이 너무 작은 경우 갱신하지도 못한 채 끝난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25019229  2.57850026  0.29134874]\n",
      " [ 0.03045833 -1.10312111  3.11782227]] (2, 3)\n",
      "[-0.12270287  0.55429116  2.98084928]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from mymodules.functions import softmax, cross_entropy_error\n",
    "from mymodules.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        # np.random.randn(int or int, int) : 적는 숫자의 shape으로 정규분포 배열을 생성해준다.\n",
    "        # ex) np.random.randn(6) : shape이 (6, )인 정규분포 배열 생성\n",
    "        # ex) np.random.randn(3, 2) : shape이 (3, 2)인 정규분포 2차원 배열 생성\n",
    "        self.W = np.random.randn(2, 3) # W : shape가 (2, 3)인 가중치 매개변수. 정규분포로 정규화한다.\n",
    "    \n",
    "    def predict(self, x): \n",
    "        return np.dot(x, self.W) # 입력과 가중치에 대한 가중합\n",
    "    \n",
    "    def loss(self, x, t): \n",
    "        z = self.predict(x) # 가중합 \n",
    "        y = softmax(z) # 출력신호를 생성하고\n",
    "        loss = cross_entropy_error(y, t) # 손실값을 계산한다.\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "net = simpleNet() # 클래스 객체를 생성하고\n",
    "print(net.W, net.W.shape)\n",
    "x = np.array([0.6, 0.9]) # 입력데이터를 준비한다.\n",
    "p = net.predict(x) # 가중합을 계산\n",
    "print(p)\n",
    "print(np.argmax(p)) # 최대값의 인덱스 : 2 ➡️ 정답이 2번 인덱스에 해당하는 값에 들어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.02376718,  0.0467727 , -0.07053987],\n",
       "       [ 0.03565077,  0.07015904, -0.10580981]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1]) # 정답 레이블 생성\n",
    "net.loss(x, t) # 손실값 계산\n",
    "\n",
    "# 기울기를 구해보자.\n",
    "def f(W): # net.W를 받아 손실값을 계산하는 함수 정의\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W) # 정의한 함수를 numerical_gradient로 전달\n",
    "print(dW.shape)\n",
    "dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 알고리즘 구현하기\n",
    "지금까지 해온 과정을 보면 아래와 같다.\n",
    "\n",
    "- 전제 : 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 \"학습\"이라고 한다. 아래의 과정으로 진행한다.\n",
    "    + 1. mini batch : 훈련 데이터 중 일부를 무작위로 가져온다. 미니 배치의 loss function value를 줄이는 것이 목표이다.\n",
    "    + 2. 기울기 산출 : 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구합니다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시합니다.\n",
    "    + 3. 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신합니다.\n",
    "    + 4. 반복 : 1 ~ 3 단계를 반복합니다.\n",
    "\n",
    "이는 경사 하강법으로 매개변수를 갱신하는 방법이며 이 때 데이터를 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법**이라고 부릅니다. 확률적으로 무작위로 골라낸 데이터에 대해 수행하는 경사 하강법이라는 의미입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망 클래스 구현하기\n",
    "from mymodules.functions import *\n",
    "from mymodules.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {} # 가중치 초기화\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size, hidden_size) # (input size, hidden size)의 크기로 정규분포를 따르는 배열(가중치)을 생성하되, 표준화를 진행\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size) # hidden size만큼 0으로 이루어진 배열(편향값) 생성\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params[\"W1\"], self.params[\"W2\"]\n",
    "        b1, b2 = self.params[\"b1\"], self.params[\"b2\"]\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1 # 은닉층 전달 가중합 계산\n",
    "        z1 = sigmoid(a1) # 활성화\n",
    "        a2 = np.dot(z1, W2) + b2 # 출령층 전달 가중합 계산\n",
    "        y = softmax(a2) # 예측 출력\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(y, axos = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads[\"W1\"] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads[\"b1\"] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads[\"W2\"] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads[\"b2\"] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet 클래스가 사용하는 변수\n",
    "\n",
    "|변수|설명|\n",
    "|---|---|\n",
    "|params|신경망의 매개변수를 보관하는 딕셔너리 변수(인스턴스 변수) <br>params[\"W1\"]은 1번째 층의 가중치, params[\"b1\"]은 1번째 층의 편향 <br> params[\"W2\"]은 2번째 층의 가중치, params[\"b2\"]은 2번째 층의 편향|\n",
    "|grads|기울기를 보관하는 딕셔너리 변수(numerical_gradient() 메소드의 반환값) <br> grads[\"W1\"]은 1번째 층의 가중치의 기울기, grads[\"b1\"]은 1번째 층의 편향값의 기울기 <br>grads[\"W2\"]은 2번째 층의 가중치의 기울기, grads[\"b2\"]은 2번째 층의 편향값의 기울기|\n",
    "\n",
    "TwoLayerNet 클래스의 메소드\n",
    "\n",
    "|메소드|설명|\n",
    "|---|---|\n",
    "|`__init__(self, input_size, hidden_size, output_size)`|초기화를 진행한다. 인수는 순서대로 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수|\n",
    "|predict(self, x)|예측(추론)을 진행한다. 인수 x는 이미지 데이터|\n",
    "|loss(self, x, t)|손실함수의 값을 구한다. 인수 x는 이미지 데이터, t는 정답 레이블(아래 세 개의 메소드도 동일)|\n",
    "|accuracy(self, x, t)|정확도를 구한다.|\n",
    "|numerical_gradient(self, x, t)|가중치 매개변수의 기울기를 구한다.|\n",
    "|gradient(self, x, t)|가중치 매개변수의 기울기를 구한다. 위 메소드의 성능이 개선된 버전이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100, 10)\n",
      "(100,)\n",
      "(10,)\n",
      "(784, 100)\n",
      "(100, 10)\n",
      "(100,)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# params 변수에는 이 신경망에 필요한 매개변수가 모두 저장됩니다. 그리고 params 변수에 저장된 가중치 매개변수가 예측 처리(순방향 처리)에서 사용됩니다.\n",
    "# 예측처리 예시\n",
    "net = TwoLayerNet(784, 100, 10)\n",
    "print(net.params[\"W1\"].shape) # 입력 -> 은닉층 가중치\n",
    "print(net.params[\"W2\"].shape) # 은닉 -> 출력층 가중치\n",
    "print(net.params[\"b1\"].shape) # 은닉층 편향값\n",
    "print(net.params[\"b2\"].shape) # 출력층 편향값\n",
    "\n",
    "x = np.random.rand(100, 784) # input shape를 맞춰주기 위해 100개의 데이터를 임시로 생성하되, 784(원래는 28x28)열의 데이터로 생성\n",
    "y = net.predict(x)\n",
    "\n",
    "# grads 변수에는 params 변수에 대응하는 각 매개변수의 기울기가 저장됩니다. 예를 들어 다음과 같이 numerical_gradient() 메서드를 사용해 기울기를 계산하면 grads 변수에 기울기 정보가 저장됩니다.\n",
    "x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량)\n",
    "t = np.random.rand(100, 10) # 더미 정답 레이블(100장 분량) \n",
    "grads = net.numerical_gradient(x, t)\n",
    "print(grads[\"W1\"].shape)\n",
    "print(grads[\"W2\"].shape)\n",
    "print(grads[\"b1\"].shape)\n",
    "print(grads[\"b2\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 미니배치 학습 구현하기\n",
    "from mymodules.mnist import load_mnist\n",
    "\n",
    "(X_train, t_train), (X_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# 하이퍼 파라미터\n",
    "n_iters = 1000 # 반복 횟수\n",
    "train_size = X_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(784, 50, 10)\n",
    "\n",
    "for i in range(n_iters):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = X_train[batch_mask]\n",
    "    y_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, y_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in (\"W1\", \"b1\", \"W2\", \"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, y_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
